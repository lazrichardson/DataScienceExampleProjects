{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag of Words and Latent Semantic Analysis\n",
    "Luther Richardson\n",
    "4-28-21\n",
    "\n",
    "This project uses the the Indiegogo Dataset (https://webrobots.io/indiegogo-dataset/) to download at least 5 files. It takes the “title” for each project in the dataset. \n",
    "\n",
    "Then, the article titles are put into a “bag of words” format. Finally, Latent Semantic Analysis (LSA) is used to cluser them into related topics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/luther/opt/anaconda3/lib/python3.8/site-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/luther/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import modules\n",
    "import os.path\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "from gensim import models\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "files =[\"/Users/luther/Desktop/indie/2020-11-Indiegogo.csv\",\n",
    "\"/Users/luther/Desktop/indie/2020-11-Indiegogo001.csv\",\n",
    "\"/Users/luther/Desktop/indie/2020-11-Indiegogo002.csv\",\n",
    "\"/Users/luther/Desktop/indie/2020-12-Indiegogo001.csv\",\n",
    "\"/Users/luther/Desktop/indie/2020-12-Indiegogo002.csv\",\n",
    "\"/Users/luther/Desktop/indie/2021-02-Indiegogo002.csv\",\n",
    "\"/Users/luther/Desktop/indie/2020-11-Indiegogo.csv\",\n",
    "\"/Users/luther/Desktop/indie/2020-11-Indiegogo001.csv\",\n",
    "\"/Users/luther/Desktop/indie/2020-11-Indiegogo002.csv\",\n",
    "\"/Users/luther/Desktop/indie/2020-12-Indiegogo.csv\",\n",
    "\"/Users/luther/Desktop/indie/2020-12-Indiegogo001.csv\",\n",
    "\"/Users/luther/Desktop/indie/2020-12-Indiegogo002.csv\",\n",
    "\"/Users/luther/Desktop/indie/2021-01-Indiegogo.csv\",\n",
    "\"/Users/luther/Desktop/indie/2021-01-Indiegogo001.csv\",\n",
    "\"/Users/luther/Desktop/indie/2021-01-Indiegogo002.csv\",\n",
    "\"/Users/luther/Desktop/indie/2021-02-Indiegogo.csv\",\n",
    "\"/Users/luther/Desktop/indie/2021-02-Indiegogo001.csv\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the data\n",
    "def load_data(path, column):\n",
    "    data = pd.read_csv(path,\n",
    "                       low_memory=False)\n",
    "    data = data[column]\n",
    "    documents = data.values.tolist()\n",
    "    print(\"Documents:\",len(data))\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents: 32876\n",
      "Documents: 32856\n",
      "Documents: 1241\n",
      "Documents: 32845\n",
      "Documents: 1495\n",
      "Documents: 1676\n",
      "Documents: 32876\n",
      "Documents: 32856\n",
      "Documents: 1241\n",
      "Documents: 32848\n",
      "Documents: 32845\n",
      "Documents: 1495\n",
      "Documents: 32859\n",
      "Documents: 32854\n",
      "Documents: 1544\n",
      "Documents: 32855\n",
      "Documents: 32855\n"
     ]
    }
   ],
   "source": [
    "# import the dataset\n",
    "documents = []\n",
    "\n",
    "for file in files:\n",
    "    data = load_data(file,'title')\n",
    "    for title in data:\n",
    "        documents.append(title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How many documents?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "370117"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The Scoutmother', 'Feet on the Ground Scholarship Fund ', 'Friday the 13th: LOST - A fan film', 'Float with Pierre King!', 'Real Spanish', 'Historical Ecology of Onondaga Lake', 'LANGRIA 6-in-1 Astronaut Memory Foam Travel Pillow', 'Beloved Magazine, The Gather Issue', 'Female Fitness and Bodybuilding Photoshootings', \"Birdie's Plus Size Thrift Store\"]\n"
     ]
    }
   ],
   "source": [
    "print(documents[0:10]) # examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start creating the bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_doc(item):\n",
    "    item = re.sub(\"\\W|[0-9]\", \" \", item) # remove non alpha\n",
    "    item = re.sub(\"\\s+\", \" \", item) # remove more than one space\n",
    "    item =  item.lower() # lowercase \n",
    "    return str(item)\n",
    "\n",
    "tokenized_docs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize\n",
    "stop_words = set(stopwords.words('english'))\n",
    "for doc in documents:\n",
    "    # tokenize removing len less than 3 and over 15 char\n",
    "    doc = clean_doc(str(doc))\n",
    "    doc = gensim.utils.simple_preprocess(doc, min_len=3, max_len=15)\n",
    "    removed_stops = []\n",
    "    for word in doc:\n",
    "        if (word not in stop_words):\n",
    "            removed_stops.append(word)\n",
    "    tokenized_docs.append(removed_stops)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example of tokenized docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['scoutmother'], ['feet', 'ground', 'scholarship', 'fund'], ['friday', 'lost', 'fan', 'film'], ['float', 'pierre', 'king'], ['real', 'spanish'], ['historical', 'ecology', 'onondaga', 'lake'], ['langria', 'astronaut', 'memory', 'foam', 'travel', 'pillow'], ['beloved', 'magazine', 'gather', 'issue'], ['female', 'fitness', 'bodybuilding', 'photoshootings'], ['birdie', 'plus', 'size', 'thrift', 'store']]\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_docs[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the term dictionary out of the tokenized docs\n",
    "### Then, converting list of documents (corpus) into Document Term Matrix using dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = corpora.Dictionary(tokenized_docs)\n",
    "doc_term_matrix = [dictionary.doc2bow(document) for document in tokenized_docs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of words representation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(0, 1)], [(1, 1), (2, 1), (3, 1), (4, 1)], [(5, 1), (6, 1), (7, 1), (8, 1)], [(9, 1), (10, 1), (11, 1)], [(12, 1), (13, 1)], [(14, 1), (15, 1), (16, 1), (17, 1)], [(18, 1), (19, 1), (20, 1), (21, 1), (22, 1), (23, 1)], [(24, 1), (25, 1), (26, 1), (27, 1)], [(28, 1), (29, 1), (30, 1), (31, 1)], [(32, 1), (33, 1), (34, 1), (35, 1), (36, 1)]]\n"
     ]
    }
   ],
   "source": [
    "print(doc_term_matrix[0:10]) # printing 10 rows for example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the LDA model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tfidf stands for \"term frequency–inverse document frequency\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = models.TfidfModel(doc_term_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_tfidf = tfidf[doc_term_matrix] # convert doc term matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize an LSI transformation\n",
    "lsi_model = models.LsiModel(corpus_tfidf, id2word=dictionary, num_topics=15)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = lsi_model.get_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " film short help feature project series new world horror first \n",
      " \n",
      " help new world project film camera first short series album \n",
      " \n",
      " series web help project book camera new world season album \n",
      " \n",
      " project series web help new world first album art photography \n",
      " \n",
      " world help first project smart bike wireless camera smallest one \n",
      " \n",
      " new help album world camera debut first series game save \n",
      " \n",
      " game card board book world project new camera album video \n",
      " \n",
      " book game comic photography help photo project series web children \n",
      " \n",
      " camera album debut bike music fund action first world lens \n",
      " \n",
      " bike world album smart one electric life power art first \n",
      " \n",
      " art bike music festival electric book album project studio one \n",
      " \n",
      " one life bike album music new campaign wireless world smart \n",
      " \n",
      " life bike art electric app save camera album new wireless \n",
      " \n",
      " campaign one smart life season app home wireless photography world \n",
      " \n",
      " one photography music album smart life campaign new debut art \n",
      " \n"
     ]
    }
   ],
   "source": [
    "for item in lsi_model.show_topics():\n",
    "    print(clean_doc(item[1]))\n",
    "    print(\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find what topics are associated with each document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_lsi = lsi_model[corpus_tfidf] # get the LSI model from the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the topics and docs out into arrays\n",
    "topics = []\n",
    "docs = []\n",
    "\n",
    "for doc, as_text in zip(corpus_lsi, documents):\n",
    "    topics.append(doc) \n",
    "    docs.append(as_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the highest scoring topic for each document\n",
    "topic_vals = []\n",
    "\n",
    "for topic in topics:\n",
    "    max_val = -99999999.0\n",
    "    i = 0\n",
    "    top_index = 0\n",
    "    for value in topic:\n",
    "        if(np.all(value[1] > max_val)):\n",
    "            max_val = value \n",
    "            top_index = i\n",
    "        i += 1\n",
    "    topic_vals.append(top_index)\n",
    "    #print(max_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 4, 0, 2, 2, 0, 13, 6, 2, 11]\n"
     ]
    }
   ],
   "source": [
    "print(topic_vals[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {'Topic':topic_vals,\n",
    "        'Doc':docs}\n",
    "data = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Topic                                                Doc\n",
      "0           1                                    The Scoutmother\n",
      "1           4               Feet on the Ground Scholarship Fund \n",
      "2           0                 Friday the 13th: LOST - A fan film\n",
      "3           2                            Float with Pierre King!\n",
      "4           2                                       Real Spanish\n",
      "...       ...                                                ...\n",
      "370112      3  Frog Conservation Research at Tarleton State U...\n",
      "370113     11  Decisions: A Minor Variation - A Special One-Shot\n",
      "370114      2                          INFERTILITY TO FERTILITY!\n",
      "370115      2                             Stay Nerdy Productions\n",
      "370116      2  The Invincible Osiris Jackson: A Gaymer Web Se...\n",
      "\n",
      "[370117 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9    68435\n",
       "6    58433\n",
       "2    54824\n",
       "0    48259\n",
       "5    46191\n",
       "8    31271\n",
       "4    30078\n",
       "3    24996\n",
       "1     6717\n",
       "7      913\n",
       "Name: Topic, dtype: int64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Topic'].value_counts(normalize=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
